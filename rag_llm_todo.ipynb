{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu7Fp4WiIjMO"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade google-cloud-aiplatform\n",
        "# !pip install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vhl_UaD_dVA"
      },
      "source": [
        "## Deploying Gemma to Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337B0bt531Dz"
      },
      "outputs": [],
      "source": [
        "GCP_PROJECT_ID = 'mbd-2024-gemma'\n",
        "PROJECT_NUMBER = '743869327957'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n81CJNW54GkW"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=GCP_PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0StETUAO3p16"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Union\n",
        "from google.cloud import aiplatform\n",
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value\n",
        "\n",
        "def predict_custom_trained_model_sample(\n",
        "    project: str,\n",
        "    endpoint_id: str,\n",
        "    instances: Union[Dict, List[Dict]],\n",
        "    location: str = \"us-central1\",\n",
        "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
        "):\n",
        "    \"\"\"\n",
        "    `instances` can be either single instance of type dict or a list\n",
        "    of instances.\n",
        "    \"\"\"\n",
        "    client_options = {\"api_endpoint\": api_endpoint}\n",
        "\n",
        "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
        "\n",
        "    instances = instances if isinstance(instances, list) else [instances]\n",
        "\n",
        "    instances = [\n",
        "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
        "    ]\n",
        "\n",
        "    parameters_dict = {}\n",
        "\n",
        "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
        "\n",
        "    endpoint = client.endpoint_path(\n",
        "        project=project, location=location, endpoint=endpoint_id\n",
        "    )\n",
        "\n",
        "\n",
        "    response = client.predict(\n",
        "        endpoint=endpoint, instances=instances, parameters=parameters\n",
        "    )\n",
        "\n",
        "    res_list = list(response.predictions)\n",
        "    prediction = res_list[0].split('\\nOutput:\\n')[-1]\n",
        "\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmhyXb565yFi"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCaIdBIJ3z_W"
      },
      "outputs": [],
      "source": [
        "res = predict_custom_trained_model_sample(\n",
        "    project=\"743869327957\",\n",
        "    endpoint_id=\"3561003702040920064\",\n",
        "    location=\"us-central1\",\n",
        "    instances=instances\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1spS3c0BDBU"
      },
      "source": [
        "## Vector Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYJf16SyBiE-"
      },
      "outputs": [],
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "def text_embedding(text_input) -> list:\n",
        "    \"\"\"\n",
        "    Text embedding with a Large Language Model.\n",
        "\n",
        "    Args:\n",
        "      - text_input (str).\n",
        "\n",
        "    Returns\n",
        "      - vector (List[floats]).\n",
        "\n",
        "    \"\"\"\n",
        "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "    embeddings = model.get_embeddings([text_input])\n",
        "    for embedding in embeddings:\n",
        "        vector = embedding.values\n",
        "    return vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SMldSJpCdVy"
      },
      "source": [
        "# Storing embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1. Find a data structure and function that can store text chunks and their embeddings. Write a function to easily add a new text chunk to that data structure."
      ],
      "metadata": {
        "id": "idDYBvFdS1Z9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOvvtkNcCnHM"
      },
      "outputs": [],
      "source": [
        "vector_memory = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXykBxy3CsVl"
      },
      "outputs": [],
      "source": [
        "def add_chunk(text_string, vector_memory=vector_memory):\n",
        "  \"\"\"\n",
        "  Adds new text to vector memory.\n",
        "\n",
        "  Args:\n",
        "    - text_string (str). The text we want to embedd and store.\n",
        "    - vector_memory (Dict) Dictionary mapping from text_strings to embedding vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwhqZ_7dTv9n"
      },
      "source": [
        "# Retrieving embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2. Write a function to retrieve the k most similar embeddings from that data structure."
      ],
      "metadata": {
        "id": "05wGfLrsTHZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfEImTEFDePL"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(query, k, vector_memory=vector_memory):\n",
        "  \"\"\"\n",
        "  Given a query string, retrieves the k most similar vectors.\n",
        "\n",
        "  Args:\n",
        "    - query (string).\n",
        "    - k (int)\n",
        "    - vector_memory (Dict). Maps of text and embeddings\n",
        "\n",
        "  Returns:\n",
        "    - k most similar text strings.\n",
        "  \"\"\"\n",
        "\n",
        "  #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rP1rTwnTxup"
      },
      "source": [
        "# Loading PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3. Write a function that takes a filename, extracts the text of a pdf document, and splits it into chunks. You can use pypdf for that."
      ],
      "metadata": {
        "id": "ON3TO3LxTQO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTVNmPf7J1RN"
      },
      "outputs": [],
      "source": [
        "import pypdf\n",
        "\n",
        "def extract_text_from_pdf(filename):\n",
        "  \"\"\"\n",
        "  Loads a pdf file and chunks it.\n",
        "\n",
        "  Args:\n",
        "    - filen_mae (str): path to PDF file.\n",
        "\n",
        "  Returns\n",
        "  \"\"\"\n",
        "  with open(filename, 'rb') as pdf_file:\n",
        "\n",
        "    pdf_reader = pypdf.PdfReader(pdf_file)\n",
        "\n",
        "    #TODO\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_IsntpTINt"
      },
      "source": [
        "### Task 4. Load text chunks, create embeddings and save them in your vector data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bckVPPhkTfXN"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egn_lJlAUOMJ"
      },
      "source": [
        "### Task 5. Get a question from the user and find the most relevant context from the PDF you loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo_VNvl_UUN9"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldz6EEo-UnC0"
      },
      "source": [
        "### Task 6. Use prompt engineering and the Gemma LLM to answer the question based on the context you retrieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmAv6KPoUsIm"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aghOXsDuUje0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}